{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e96a4e39",
   "metadata": {},
   "source": [
    "## Transformer Implementation with PyTorch\n",
    "\n",
    "##### Source: Daniel Melchor (dmh672@gmail.com)\n",
    "\n",
    "### Here a Transformer Network is implemented which is being trained on random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee964123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e051702",
   "metadata": {},
   "source": [
    "## 1. Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "291696bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim_model, dropout_p, max_len):\n",
    "        super().__init__()\n",
    "        # Modified version from: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "        # max_len determines how far the position can have an effect on a token (window)\n",
    "        \n",
    "        # Info\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        # Encoding - From formula\n",
    "        pos_encoding = torch.zeros(max_len, dim_model)\n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1) # 0, 1, 2, 3, 4, 5\n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model) # 1000^(2i/dim_model)\n",
    "        \n",
    "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        \n",
    "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "        # Saving buffer (same as parameter without gradients needed)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
    "        \n",
    "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
    "        # Residual connection + pos encoding\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68e7313",
   "metadata": {},
   "source": [
    "## 2. Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd7b76a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Model from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: https://medium.com/p/c80afbc9ffb1/\n",
    "    \"\"\"\n",
    "    # Constructor\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_tokens,\n",
    "        dim_model,\n",
    "        num_heads,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        dropout_p,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # INFO\n",
    "        self.model_type = \"Transformer\"\n",
    "        self.dim_model = dim_model\n",
    "\n",
    "        # LAYERS\n",
    "        self.positional_encoder = PositionalEncoding(\n",
    "            dim_model=dim_model, dropout_p=dropout_p, max_len=5000\n",
    "        )\n",
    "        self.embedding = nn.Embedding(num_tokens, dim_model)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=dim_model,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dropout=dropout_p,\n",
    "        )\n",
    "        self.out = nn.Linear(dim_model, num_tokens)\n",
    "        \n",
    "    def forward(self, src, tgt, tgt_mask=None, src_pad_mask=None, tgt_pad_mask=None):\n",
    "        # Src size must be (batch_size, src sequence length)\n",
    "        # Tgt size must be (batch_size, tgt sequence length)\n",
    "\n",
    "        # Embedding + positional encoding - Out size = (batch_size, sequence length, dim_model)\n",
    "        src = self.embedding(src) * math.sqrt(self.dim_model)\n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.dim_model)\n",
    "        src = self.positional_encoder(src)\n",
    "        tgt = self.positional_encoder(tgt)\n",
    "        \n",
    "        # We could use the parameter batch_first=True, but our KDL version doesn't support it yet, so we permute\n",
    "        # to obtain size (sequence length, batch_size, dim_model),\n",
    "        src = src.permute(1,0,2)\n",
    "        tgt = tgt.permute(1,0,2)\n",
    "\n",
    "        # Transformer blocks - Out size = (sequence length, batch_size, num_tokens)\n",
    "        transformer_out = self.transformer(src, tgt, tgt_mask=tgt_mask, src_key_padding_mask=src_pad_mask, tgt_key_padding_mask=tgt_pad_mask)\n",
    "        out = self.out(transformer_out)\n",
    "        \n",
    "        return out\n",
    "      \n",
    "    def get_tgt_mask(self, size) -> torch.tensor:\n",
    "        # Generates a squeare matrix where the each row allows one word more to be seen\n",
    "        mask = torch.tril(torch.ones(size, size) == 1) # Lower triangular matrix\n",
    "        mask = mask.float()\n",
    "        mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf\n",
    "        mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0\n",
    "        \n",
    "        # EX for size=5:\n",
    "        # [[0., -inf, -inf, -inf, -inf],\n",
    "        #  [0.,   0., -inf, -inf, -inf],\n",
    "        #  [0.,   0.,   0., -inf, -inf],\n",
    "        #  [0.,   0.,   0.,   0., -inf],\n",
    "        #  [0.,   0.,   0.,   0.,   0.]]\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    def create_pad_mask(self, matrix: torch.tensor, pad_token: int) -> torch.tensor:\n",
    "        # If matrix = [1,2,3,0,0,0] where pad_token=0, the result mask is\n",
    "        # [False, False, False, True, True, True]\n",
    "        return (matrix == pad_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bfcd77",
   "metadata": {},
   "source": [
    "## 3. Generate Random Data and Batchify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4dc0431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_data(n):\n",
    "    SOS_token = np.array([2])\n",
    "    EOS_token = np.array([3])\n",
    "    length = 8\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # 1,1,1,1,1,1 -> 1,1,1,1,1\n",
    "    for i in range(n // 3):\n",
    "        X = np.concatenate((SOS_token, np.ones(length), EOS_token))\n",
    "        y = np.concatenate((SOS_token, np.ones(length), EOS_token))\n",
    "        data.append([X, y])\n",
    "\n",
    "    # 0,0,0,0 -> 0,0,0,0\n",
    "    for i in range(n // 3):\n",
    "        X = np.concatenate((SOS_token, np.zeros(length), EOS_token))\n",
    "        y = np.concatenate((SOS_token, np.zeros(length), EOS_token))\n",
    "        data.append([X, y])\n",
    "\n",
    "    # 1,0,1,0 -> 1,0,1,0,1\n",
    "    for i in range(n // 3):\n",
    "        X = np.zeros(length)\n",
    "        start = random.randint(0, 1)\n",
    "\n",
    "        X[start::2] = 1\n",
    "\n",
    "        y = np.zeros(length)\n",
    "        if X[-1] == 0:\n",
    "            y[::2] = 1\n",
    "        else:\n",
    "            y[1::2] = 1\n",
    "\n",
    "        X = np.concatenate((SOS_token, X, EOS_token))\n",
    "        y = np.concatenate((SOS_token, y, EOS_token))\n",
    "\n",
    "        data.append([X, y])\n",
    "\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def batchify_data(data, batch_size=16, padding=False, padding_token=-1):\n",
    "    batches = []\n",
    "    for idx in range(0, len(data), batch_size):\n",
    "        # We make sure we dont get the last bit if its not batch_size size\n",
    "        if idx + batch_size < len(data):\n",
    "            # Here you would need to get the max length of the batch,\n",
    "            # and normalize the length with the PAD token.\n",
    "            if padding:\n",
    "                max_batch_length = 0\n",
    "\n",
    "                # Get longest sentence in batch\n",
    "                for seq in data[idx : idx + batch_size]:\n",
    "                    if len(seq) > max_batch_length:\n",
    "                        max_batch_length = len(seq)\n",
    "\n",
    "                # Append X padding tokens until it reaches the max length\n",
    "                for seq_idx in range(batch_size):\n",
    "                    remaining_length = max_bath_length - len(data[idx + seq_idx])\n",
    "                    data[idx + seq_idx] += [padding_token] * remaining_length\n",
    "\n",
    "            batches.append(np.array(data[idx : idx + batch_size]).astype(np.int64))\n",
    "\n",
    "    print(f\"{len(batches)} batches of size {batch_size}\")\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04246ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "562 batches of size 16\n",
      "187 batches of size 16\n"
     ]
    }
   ],
   "source": [
    "# Generating random Data\n",
    "train_data = generate_random_data(9000)\n",
    "val_data = generate_random_data(3000)\n",
    "\n",
    "# Train and Validation Dataloader\n",
    "train_dataloader = batchify_data(train_data)\n",
    "val_dataloader = batchify_data(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26d0dcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4ab26c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (positional_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (embedding): Embedding(4, 8)\n",
      "  (transformer): Transformer(\n",
      "    (encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=8, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=8, bias=True)\n",
      "          (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=8, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=8, bias=True)\n",
      "          (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=8, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=8, bias=True)\n",
      "          (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): TransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=8, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=8, bias=True)\n",
      "          (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=8, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=8, bias=True)\n",
      "          (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=8, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=8, bias=True)\n",
      "          (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (out): Linear(in_features=8, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Creating a Model Objectg with relevent Network Structure\n",
    "model = Transformer(\n",
    "    num_tokens=4, dim_model=8, num_heads=2, num_encoder_layers=3, num_decoder_layers=3, dropout_p=0.1\n",
    ").to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08a0c513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Optimiser and Loss Function\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922eb471",
   "metadata": {},
   "source": [
    "## 4. Training and Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ec4bbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, opt, loss_fn, dataloader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        X, y = batch[:, 0], batch[:, 1]\n",
    "        X, y = torch.tensor(X).to(device), torch.tensor(y).to(device)\n",
    "\n",
    "        # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
    "        y_input = y[:,:-1]\n",
    "        y_expected = y[:,1:]\n",
    "        \n",
    "        # Get mask to mask out the next words\n",
    "        sequence_length = y_input.size(1)\n",
    "        tgt_mask = model.get_tgt_mask(sequence_length).to(device)\n",
    "\n",
    "        # Standard training except we pass in y_input and tgt_mask\n",
    "        pred = model(X, y_input, tgt_mask)\n",
    "\n",
    "        # Permute pred to have batch size first again\n",
    "        pred = pred.permute(1, 2, 0)      \n",
    "        loss = loss_fn(pred, y_expected)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    \n",
    "        total_loss += loss.detach().item()\n",
    "        \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validation_loop(model, loss_fn, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            X, y = batch[:, 0], batch[:, 1]\n",
    "            X, y = torch.tensor(X, dtype=torch.long, device=device), torch.tensor(y, dtype=torch.long, device=device)\n",
    "\n",
    "            # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
    "            y_input = y[:,:-1]\n",
    "            y_expected = y[:,1:]\n",
    "            \n",
    "            # Get mask to mask out the next words\n",
    "            sequence_length = y_input.size(1)\n",
    "            tgt_mask = model.get_tgt_mask(sequence_length).to(device)\n",
    "\n",
    "            # Standard training except we pass in y_input and src_mask\n",
    "            pred = model(X, y_input, tgt_mask)\n",
    "\n",
    "            # Permute pred to have batch size first again\n",
    "            pred = pred.permute(1, 2, 0)      \n",
    "            loss = loss_fn(pred, y_expected)\n",
    "            total_loss += loss.detach().item()\n",
    "        \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e05a9159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, opt, loss_fn, train_dataloader, val_dataloader, epochs):\n",
    "    # Used for plotting later on\n",
    "    train_loss_list, validation_loss_list = [], []\n",
    "    \n",
    "    print(\"Training and validating model\")\n",
    "    for epoch in range(epochs):\n",
    "        print(\"-\"*25, f\"Epoch {epoch + 1}\",\"-\"*25)\n",
    "        \n",
    "        train_loss = train_loop(model, opt, loss_fn, train_dataloader)\n",
    "        train_loss_list += [train_loss]\n",
    "        \n",
    "        validation_loss = validation_loop(model, loss_fn, val_dataloader)\n",
    "        validation_loss_list += [validation_loss]\n",
    "        \n",
    "        print(f\"Training loss: {train_loss:.4f}\")\n",
    "        print(f\"Validation loss: {validation_loss:.4f}\")\n",
    "        print()\n",
    "    return train_loss_list, validation_loss_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bb4437",
   "metadata": {},
   "source": [
    "## 5. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7dd68544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and validating model\n",
      "------------------------- Epoch 1 -------------------------\n",
      "Training loss: 0.5385\n",
      "Validation loss: 0.4012\n",
      "\n",
      "------------------------- Epoch 2 -------------------------\n",
      "Training loss: 0.4157\n",
      "Validation loss: 0.3746\n",
      "\n",
      "------------------------- Epoch 3 -------------------------\n",
      "Training loss: 0.3797\n",
      "Validation loss: 0.3214\n",
      "\n",
      "------------------------- Epoch 4 -------------------------\n",
      "Training loss: 0.3375\n",
      "Validation loss: 0.2730\n",
      "\n",
      "------------------------- Epoch 5 -------------------------\n",
      "Training loss: 0.3052\n",
      "Validation loss: 0.2289\n",
      "\n",
      "------------------------- Epoch 6 -------------------------\n",
      "Training loss: 0.2805\n",
      "Validation loss: 0.2146\n",
      "\n",
      "------------------------- Epoch 7 -------------------------\n",
      "Training loss: 0.2653\n",
      "Validation loss: 0.1961\n",
      "\n",
      "------------------------- Epoch 8 -------------------------\n",
      "Training loss: 0.2518\n",
      "Validation loss: 0.1782\n",
      "\n",
      "------------------------- Epoch 9 -------------------------\n",
      "Training loss: 0.2391\n",
      "Validation loss: 0.1731\n",
      "\n",
      "------------------------- Epoch 10 -------------------------\n",
      "Training loss: 0.2323\n",
      "Validation loss: 0.1664\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_loss_list, validation_loss_list = fit(model, optimizer, loss_function, train_dataloader, val_dataloader, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eac731",
   "metadata": {},
   "source": [
    "## 6. Checking Model and Hyperparamter Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c2b4d974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss_list, label = \"Train loss\")\n",
    "plt.plot(validation_loss_list, label = \"Validation loss\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss vs Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940ee7fc",
   "metadata": {},
   "source": [
    "## 7. Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ce47802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, input_sequence, max_length=15, SOS_token=2, EOS_token=3):\n",
    "    model.eval()\n",
    "    \n",
    "    y_input = torch.tensor([[SOS_token]], dtype=torch.long, device=device)\n",
    "\n",
    "    num_tokens = len(input_sequence[0])\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        # Get source mask\n",
    "        tgt_mask = model.get_tgt_mask(y_input.size(1)).to(device)\n",
    "        \n",
    "        pred = model(input_sequence, y_input, tgt_mask)\n",
    "        \n",
    "        next_item = pred.topk(1)[1].view(-1)[-1].item() # num with highest probability\n",
    "        next_item = torch.tensor([[next_item]], device=device)\n",
    "\n",
    "        # Concatenate previous input with predicted best word\n",
    "        y_input = torch.cat((y_input, next_item), dim=1)\n",
    "\n",
    "        # Stop if model predicts end of sentence\n",
    "        if next_item.view(-1).item() == EOS_token:\n",
    "            break\n",
    "\n",
    "    return y_input.view(-1).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3d8b6b",
   "metadata": {},
   "source": [
    "#### Here we test some examples to observe how the model predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b948e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    torch.tensor([[2, 0, 0, 0, 0, 0, 0, 0, 0, 3]], dtype=torch.long, device=device),\n",
    "    torch.tensor([[2, 1, 1, 1, 1, 1, 1, 1, 1, 3]], dtype=torch.long, device=device),\n",
    "    torch.tensor([[2, 1, 0, 1, 0, 1, 0, 1, 0, 3]], dtype=torch.long, device=device),\n",
    "    torch.tensor([[2, 0, 1, 0, 1, 0, 1, 0, 1, 3]], dtype=torch.long, device=device),\n",
    "    torch.tensor([[2, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 3]], dtype=torch.long, device=device),\n",
    "    torch.tensor([[2, 0, 1, 3]], dtype=torch.long, device=device)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "97809b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0\n",
      "Input: [0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Continuation: [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Example 1\n",
      "Input: [1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Continuation: [1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "Example 2\n",
      "Input: [1, 0, 1, 0, 1, 0, 1, 0]\n",
      "Continuation: [0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
      "\n",
      "Example 3\n",
      "Input: [0, 1, 0, 1, 0, 1, 0, 1]\n",
      "Continuation: [0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
      "\n",
      "Example 4\n",
      "Input: [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
      "Continuation: [0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
      "\n",
      "Example 5\n",
      "Input: [0, 1]\n",
      "Continuation: [1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, example in enumerate(examples):\n",
    "    result = predict(model, example)\n",
    "    print(f\"Example {idx}\")\n",
    "    print(f\"Input: {example.view(-1).tolist()[1:-1]}\")\n",
    "    print(f\"Continuation: {result[1:-1]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a0125e",
   "metadata": {},
   "source": [
    "## 8. Visualise Model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "269d00ef",
   "metadata": {},
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "#yhat = model(batch.) \n",
    "\n",
    "#make_dot(yhat, params=dict(list(model.named_parameters()))).render(\"rnn_torchviz\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4692943",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
